{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmRBSW92njo4"
      },
      "source": [
        "## 1. Instalaci贸n de Dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQyNFal3nSkq",
        "outputId": "f0365e13-67b7-4656-e1cb-9b57f7e0c6be"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiQBA_ulnxbv",
        "outputId": "290a1871-7f2a-4fbc-b4bf-2857b4379801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ε Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Ε Unsloth Zoo will now patch everything to make training faster!\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM Total: 15.83 GB\n",
            "Unsloth version: 2026.1.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import unsloth\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"Unsloth version: {unsloth.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYgujLf1OMz5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "# Liberar memoria de Python\n",
        "gc.collect()\n",
        "# Vaciar cach茅 de la GPU\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40_bRQOWoDY_"
      },
      "source": [
        "## 2. Carga del Modelo Base con Unsloth\n",
        "\n",
        "Cargamos Qwen3-8B en formato 4-bit (QLoRA) para reducir uso de VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596,
          "referenced_widgets": [
            "106e694551624e059ac291dc9424913c",
            "26fd4049464d4d679825c262abcc871d",
            "89b098ccf76a48dba480b20744b8b546",
            "0bc788434bf041a5940f8229a2acd677",
            "a3d12ea02f6d4a119751828179e02733",
            "8ecde33880724142850701d525223a15",
            "f2b8bd064a6d42d797bfd6354c77badf",
            "76ff536d00c844929da45caeb1387e81",
            "16e37afa00f349638fa9c3fb88d63e2e",
            "3aebb4ef03a44e9db3727e0a48557a38",
            "d106f35c13f24d8e8bf873e700b8c526",
            "d12e5c7677084a83ba37980e6e3b96a6",
            "96529b1d84a14cccac1e66ab91cd805c",
            "bade3ac80eda435fb31f7f44558da9a8",
            "bdfd97c44f1d4a65938202e506b1847f",
            "2e38e3b4ed5749b48e769b9c22b790d9",
            "52dd2d19fd8948cf8be1c4002250f37e",
            "939efc0a9b8b4ebaa1a1e9b5504d7bb4",
            "dae2c348ec9e4c9792d468654c8568b5",
            "0d5f2eb91da040a1b6ef828dfc07babf",
            "03ef40288a3944118dc75e7c46c173c3",
            "1bae67a6057041f4ad376c3b9c3eccf3",
            "f0f5934416804b829ee6747d3c4a63c8",
            "73dfeb05c0f240b8acc8c9932f9f94c6",
            "085cd3d0196246b3b51e698e3c6b1233",
            "12d401ac76e447a183729f4050f8fba0",
            "1499bbc8044446dea126a73c4882583c",
            "9afbe8aec5a0439992eff5494856f1ac",
            "9c067132b1764c24a013c283040c5296",
            "080e801a329849e5b1e5f7a05237b03e",
            "fa5ad9717b1045fe92fe2190bf9b0320",
            "9c555c86940b45698fc6c3c64609c259",
            "a7b5989ca2f94ec5ab9e5c6d29522860",
            "4d489a86538e416d9adba259018640c0",
            "80c51d9ef6234ea58da09c8b6874e99f",
            "afddc3689f5043cebb91da3f115b070b",
            "a20efb67a0be47cf81a0c9cad030c9b8",
            "48746017d4f84bc08e69db1915f00442",
            "342829ff1aea43c9a06127619b131c76",
            "e7608295817d439ab396a8fdf77f936b",
            "5562d3e1ee2444bcb26584b697da8b77",
            "8ce7ce5e62fd425fa90988e6ce1f717b",
            "5ce1981677f341ad844389c7e573fd12",
            "c674b13234cb4fd5a75f8c1b7e499af4",
            "d8ad9bf13e104264b013c9e65ea84153",
            "91d1ba60bb9048e59a92f266aa7a442e",
            "221ca1a87cdd4ae6a48f6092cd37b415",
            "547e30255afa4be9a36a82bf1dcbd2a0",
            "29e9301ee45f462cafda398d3b2637bc",
            "f71ffc63669f4d178a54a22f65525a70",
            "1e13b8cdc3264c929b4684f19ebaff46",
            "877771af81f54aa38cb758f156f9a475",
            "c7ed6c33376c4ae08f38b7bcbb6021be",
            "105339f53b9d405983657511d1570d86",
            "1a5a4a88576c44cab91f5ca50ef2ab5e",
            "09172a1ce04646a4a339327e8f5420f9",
            "564f1650b6414eca8b7a3c758fad5463",
            "468d671d501b44f688a34aead5cfe4df",
            "2046772605a24c1cba7968929165f986",
            "647c5c266db5465ea6b04184dfec48a5",
            "e96d54837efb442589d7bad5ae080964",
            "a3fdf84c98f14b0e8bce215b1a4d9e54",
            "4556a98d4f004cfe9db7bc145186a4fd",
            "70f3f930cc9642d08af03ed88b581380",
            "db0feb8c064346a38fcacb0e81e28576",
            "d4d8e6ad945f4082aa16b30e03f995f2",
            "0eedab53001f48b3b1995a2fa5d671e9",
            "5a2848659b6c4bbbb2bceda20e5fba30",
            "bc259546061541048c9357fcf2dd3258",
            "afc6952de8b8467698b110e9800e9bae",
            "34e76f234a484f27bb93c597af31e912",
            "7155c5df974b4bcdb679d252f1d41eb1",
            "ca967f31a01144cb95811e02b104365d",
            "5ca19e380fd147e3aa56256e71342f3d",
            "76171c01a7724bb4ad74a198164429d7",
            "34c8b0d1acda49aea60cdfa1ebb8652d",
            "de0fee1b0c4c4188ad918d1702c27dd0",
            "35e061e8d1d4433080f31c011b0dd72c",
            "2f5119efe2fa46de8dc706c47fce9376",
            "a714413071c847cd8b55a6ad1b00e274",
            "88084629c9a64403a0ea248462a2b6aa",
            "2ee846ef6d34454fa754962f8bd268a5",
            "3272001a54294c5ba8bfffbf90ba5383",
            "119219804967445cb1b28d1caaf5b9bc",
            "4429ad6814e34521a3ff0490aa4da12a",
            "0c76c7c3b03349c28899d43ffb98b38c",
            "6c2f3192aec94c858d39aa69dc742c4b",
            "3f4441405d5647549ce6b904f250a707",
            "92c015f15a9743e4afc668b1ad4cf11a",
            "01ae195a7e9742d4b9d82f07af708cc6",
            "154c84b26e8549a58b8bd8fb4496d1b0",
            "fb0116339e6745979629fbeadc604393",
            "74ad0a11b08542bdb68b70910cf82a8a",
            "54fce116cd8b4979b14aaf7b37997918",
            "87aa526d4407477f9ffa2f45fafe861d",
            "461dd666498c454fb4148fe71075bb25",
            "f5aa850bf5bc4f80995bc816e74f88b6",
            "8fc821aeed1d41feaf6509f1023c4c7b",
            "0bba21201f8f446b924bbc6b924e3714",
            "11a3dad2df7645de86069c33b2d5e35d",
            "8f5df8eb721f439099597eeb7743a8a7",
            "86da0734cd664a9db0bbc39103201f3a",
            "ac96a233ac19488383c9d857ee66f1b1",
            "aca8733b138846be9eee425adb8f48f6",
            "946ffc413562423e9a4ecd78c4b75d46",
            "e44c7538da424f53ad0ac353665c77b1",
            "fc9b50d9ead24977a8d3da4f04bf6512",
            "8324446f63f94988a2046479e1991fa2",
            "e9eb3a4183a048fcb5c1551f6800b097",
            "62b0bbe54caf44209fc2b04cbf52093f",
            "8b8a7031df3c43f1a9af7ec10680c9b7",
            "5efbeb81c44546399c1c72e7cde50a9d",
            "6567120b346d40578e473050b010c2c6",
            "46f91a7de6104e659ca1d0b3efc75f84",
            "20814d1b68e245b3a0c4e539210d79ca",
            "b4e218f0b1344f9b833b4636e13c92f4",
            "f647729c023b4548bc447f3aa3c6ba32",
            "a9cd8ca1fd43402e870bc78250d504d3",
            "4c406bd77bd4469088b16f0605c5063b",
            "2989091245cf408f917e12c0ffa924f9",
            "63281c3f80d541eba8cb179def873de0",
            "62199bdb408d4b5ab002a0af065b5bf0",
            "ef1bc4a49b114458bcaa54799999d29e",
            "b7a49ef99c2441efa6688d58e57a06c1",
            "21fde1e725fb40288bc7767826beffec",
            "bbaa12f123f34a50b55915688064fdc3",
            "92e7b887b915406f8e0f3256fd2c2b8c",
            "f2a0daa7f8b441fe9116eb9544ab7b06",
            "4c3912e71a02481ca1925e30b57e623d",
            "3995073e4d39417dabac744bbdb53d2a",
            "2648e1ae1c894f728b4475d959a3e576",
            "aa21914f63804cfe87214191cc32740e"
          ]
        },
        "id": "sI9TedwqoEMP",
        "outputId": "b70d9766-a4b6-4e78-8741-b2e3b321e0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Qwen3 patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "106e694551624e059ac291dc9424913c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d12e5c7677084a83ba37980e6e3b96a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0f5934416804b829ee6747d3c4a63c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d489a86538e416d9adba259018640c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8ad9bf13e104264b013c9e65ea84153",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09172a1ce04646a4a339327e8f5420f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eedab53001f48b3b1995a2fa5d671e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35e061e8d1d4433080f31c011b0dd72c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92c015f15a9743e4afc668b1ad4cf11a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11a3dad2df7645de86069c33b2d5e35d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b8a7031df3c43f1a9af7ec10680c9b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62199bdb408d4b5ab002a0af065b5bf0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado exitosamente!\n",
            "Par谩metros del modelo: 8.19B\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Cargar modelo y tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")\n",
        "\n",
        "print(f\"Modelo cargado exitosamente!\")\n",
        "print(f\"Par谩metros del modelo: {model.num_parameters() / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NobLPDJsLAH"
      },
      "source": [
        "## 3. Configuraci贸n de LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBn8thtPsBBo",
        "outputId": "471b2913-4c05-499e-ff3a-232b891c7be4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2026.1.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Par谩metros entrenables: 87,293,952 (1.65%)\n",
            "Par谩metros totales: 5,279,101,952\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# Ver par谩metros entrenables\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Par谩metros entrenables: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
        "print(f\"Par谩metros totales: {all_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mFswuO0svl4"
      },
      "source": [
        "## 4. Preparaci贸n del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYRy-6WAswP3"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"A continuaci贸n hay una instrucci贸n que describe una tarea, junto con una entrada que proporciona m谩s contexto. Escribe una respuesta que complete apropiadamente la solicitud.\n",
        "\n",
        "### Instrucci贸n:\n",
        "{}\n",
        "\n",
        "### Entrada:\n",
        "{}\n",
        "\n",
        "### Respuesta:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Formatea ejemplos en el formato Alpaca.\"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "f4beb12ab063441e9b5cedea210cafcb",
            "02efab6c7c034dbebd60ad5d5c2b73d4",
            "5330dcc4b7864671ad8ec6986935d77e",
            "8679d9e749d04e62bd3c0b03fa7491da",
            "6cc2ba5dd270496ea19938bb56403032",
            "2c267183719a480699a7b29a0c478488",
            "1889c215e10f4ecb8f7b54fc90ee8acf",
            "0c14a4a6186f4dabb167eea5243c0b0e",
            "3a085361ebad4ecfac86718dce9f9dfb",
            "79776bf10b8b46bda6dcc4f3899b82b9",
            "b9f7087f8ab144d5a7668f2bfae3d23d",
            "5624e4928d2e4330b8b4d49b0968926f",
            "65902c51b6b24eefa64855900657bf1e",
            "edd9fb0e36084b1a97367c253a56d15d",
            "124963ed19ae43a1b393a053242c2a2f",
            "353931fbb7a34b1aaae49c4b2d7d8e78",
            "5cb52338c54d46d6a9b7b5fab0c23637",
            "233b5ca492ef40cc84d7a60ab727748d",
            "b9fbdaf6aa914300b560521604f1b5a1",
            "a92c6169644f416cb18f3b4368095194",
            "4e3cbe7b5a944ca880e85ee6c45ac351",
            "747c89ae41514c8fa55c0d288389970a"
          ]
        },
        "id": "EGeOi6GJ6Z0F",
        "outputId": "715f06a4-04e8-4025-9c1d-ed0e45e3fe17"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4beb12ab063441e9b5cedea210cafcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5624e4928d2e4330b8b4d49b0968926f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/513 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset preparado: 513 ejemplos\n",
            "\n",
            "Ejemplo de formato:\n",
            "A continuaci贸n hay una instrucci贸n que describe una tarea, junto con una entrada que proporciona m谩s contexto. Escribe una respuesta que complete apropiadamente la solicitud.\n",
            "\n",
            "### Instrucci贸n:\n",
            "Responde como un asesor de atenci贸n al cliente, de forma clara, profesional y orientada a resolver la consulta.\n",
            "\n",
            "### Entrada:\n",
            "驴Qu茅 es un impuesto?\n",
            "\n",
            "### Respuesta:\n",
            "Es un tributo cuyo pago no origina una contraprestaci贸n directa en favor del contribuyente por parte del Estado. Un ejemplo de esto es el Impuesto a la Renta.<|im_end|>...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"dataset_finetuning.jsonl\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(f\"Dataset preparado: {len(dataset)} ejemplos\")\n",
        "print(\"\\nEjemplo de formato:\")\n",
        "print(dataset[0][\"text\"][:1000] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plpY9oPu6uog"
      },
      "source": [
        "## 5. Configuraci贸n del Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "35012b08914f4cec9414cb34f8db2c09",
            "6d7ee518549440efb495433e4fa1fd32",
            "6c10c39470cf4696a6826e5aa612acc5",
            "016c9e191c2d4b2fbe03014fa5cba27b",
            "41ca5503d02c4314841d22e29789fa29",
            "6af0b2805f7e485ab43525f08f08c469",
            "65ceece60ee648a2be8dbde1347f0f80",
            "ac4728e9a22e4368a2e6f1436bff19fb",
            "c14a55a8b0ff4af2a4c33567bdd13f85",
            "70ba5de734794034bec2f95b06dc271c",
            "ea549f9f870f4a20b22061e3045994ce"
          ]
        },
        "id": "rbQ1MlsV6wXw",
        "outputId": "571f1eeb-9a4f-4cd6-d69e-e26545f4c2ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35012b08914f4cec9414cb34f8db2c09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/513 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer configurado. Iniciando entrenamiento...\n",
            "Batch size efectivo: 8 (per_device * gradient_accumulation)\n",
            "Total steps: ~192 steps\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 3,\n",
        "        max_steps = -1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"paged_adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        gradient_checkpointing = True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Trainer configurado. Iniciando entrenamiento...\")\n",
        "print(f\"Batch size efectivo: {2 * 4} (per_device * gradient_accumulation)\")\n",
        "print(f\"Total steps: ~{len(dataset) * 3 // (2 * 4)} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaQAtNEO7buP"
      },
      "source": [
        "## 6. Entrenamiento (Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XjCU2CQL7bKI",
        "outputId": "f08671c1-02df-4eae-f76f-c4b8cb1c323f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "VRAM en uso antes del training: 7.395 GB.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 513 | Num Epochs = 3 | Total steps = 195\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 87,293,952 of 8,278,029,312 (1.05% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [195/195 19:17, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.316300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.449100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.359600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.266800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.992900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.747900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.083700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.012900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.268100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.879400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.971300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.895900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.672400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.018400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.926200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.845800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.905200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.900500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.701800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.943900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.726800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.766000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.707900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.958100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.819000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.888600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.823500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.847400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.635800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.753900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.799900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.782600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.880300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.938300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.856900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.834800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.791700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.795700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.644900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.939600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.650500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.679200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.042600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.556300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.722800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.798100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.739700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.608900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.648500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.616600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.535400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.697200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.818900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.478900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.420800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.751900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.638900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.638700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.593000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.542200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.639500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.492400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.737700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.665600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.777100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.673700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.526500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.433900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.613600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.477500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.491600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.853100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.406600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.491100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.445500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.504600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.311600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.307500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.777600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.364100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.375500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.394500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.348300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.315600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.444800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.445900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.345300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.337000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.386200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.322400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.440800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.587600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.341800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.457000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.390600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.442800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.350400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.311100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.429900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.378000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.397400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.462100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.439100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.438700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.452300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.368000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.523400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.330500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.311500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.418400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.266600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.300900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.365300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Entrenamiento completado!\n",
            "==================================================\n",
            "Tiempo total: 1222.54 segundos\n",
            "VRAM peak usada: 9.752 GB\n",
            "VRAM para LoRA: 2.357 GB\n",
            "Porcentaje usado: 66.156%\n",
            "Loss final: 0.6780\n"
          ]
        }
      ],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"VRAM en uso antes del training: {start_gpu_memory} GB.\")\n",
        "\n",
        "# Entrenar\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Estad铆sticas finales\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Entrenamiento completado!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Tiempo total: {trainer_stats.metrics['train_runtime']:.2f} segundos\")\n",
        "print(f\"VRAM peak usada: {used_memory} GB\")\n",
        "print(f\"VRAM para LoRA: {used_memory_for_lora} GB\")\n",
        "print(f\"Porcentaje usado: {used_percentage}%\")\n",
        "print(f\"Loss final: {trainer_stats.metrics['train_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQ1_lvx7iyg"
      },
      "source": [
        "## 7. Inferencia con el Modelo Fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynJX38oM7kYY",
        "outputId": "19ac6070-5b4e-4cd9-87bc-468b83c70949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo listo para inferencia!\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def generar_respuesta(instruccion, input_texto):\n",
        "    \"\"\"Genera una respuesta usando el modelo fine-tuned.\"\"\"\n",
        "    prompt = alpaca_prompt.format(\n",
        "        instruccion,\n",
        "        input_texto,\n",
        "        \"\"  # Respuesta vac铆a, el modelo la completar谩\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 1024,\n",
        "        temperature = 0.4,\n",
        "        top_p = 0.9,\n",
        "        top_k = 50,\n",
        "        use_cache = True\n",
        "    )\n",
        "\n",
        "    # Decodificar solo la respuesta generada (sin el prompt)\n",
        "    resultado_completo = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extraer solo la respuesta\n",
        "    if \"### Respuesta:\" in resultado_completo:\n",
        "        respuesta = resultado_completo.split(\"### Respuesta:\")[1].strip()\n",
        "    else:\n",
        "        respuesta = resultado_completo\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "print(\"Modelo listo para inferencia!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU7f-IE9Tqco"
      },
      "source": [
        "#Pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSTUTTHJ7oUo",
        "outputId": "116c7e86-54f2-43b9-a4d0-d9c78922e78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Cliente: 驴Qu茅 es un impuesto?\n",
            "\n",
            "Asistente: Un impuesto es un tributo cuyo pago no origina una contraprestaci贸n directa en favor del contribuyente por parte del Estado.\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "\n",
        "consulta1 = \"驴Qu茅 es un impuesto?\"\n",
        "respuesta1 = generar_respuesta(\n",
        "    \"Responde como un asesor de atenci贸n al cliente, de forma clara, profesional y orientada a resolver la consulta.\",\n",
        "    consulta1\n",
        ")\n",
        "\n",
        "print(f\"Cliente: {consulta1}\")\n",
        "print(f\"\\nAsistente: {respuesta1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnCl5LQdZeL5",
        "outputId": "06915241-5930-457a-9671-e3387014d18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Cliente: 驴Qu茅 son los tributos no vinculados?\n",
            "\n",
            "Asistente: Los tributos no vinculados son los impuestos.\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "\n",
        "consulta1 = \"驴Qu茅 son los tributos no vinculados?\"\n",
        "respuesta1 = generar_respuesta(\n",
        "    \"Responde como un asesor de atenci贸n al cliente, de forma clara, profesional y orientada a resolver la consulta.\",\n",
        "    consulta1\n",
        ")\n",
        "\n",
        "print(f\"Cliente: {consulta1}\")\n",
        "print(f\"\\nAsistente: {respuesta1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_m2v0E5ZDkx"
      },
      "source": [
        "## 8. Guardar el Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcvceH-5bKqA"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJM-J1KOZE-Y",
        "outputId": "cc9cec07-72f8-4fa3-a1fd-81e1365b693e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Adaptadores LoRA guardados en: Qwen3-4B-Sunat-v1/\n",
            "Para cargar: model = FastLanguageModel.from_pretrained('Qwen3-4B-Sunat-v1')\n"
          ]
        }
      ],
      "source": [
        "save_path = \"/content/drive/My Drive/Modelos_Qwen/Sunat_LoRA_v1\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\" Adaptadores LoRA guardados en: Qwen3-4B-Sunat-v1/\")\n",
        "print(\"Para cargar: model = FastLanguageModel.from_pretrained('Qwen3-4B-Sunat-v1')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKPSd6eSZThg"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.save_pretrained_merged(\n",
        "    \"/content/drive/My Drive/Modelos_Qwen/Sunat_Merged_16bit\",\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "\n",
        "print(\" Modelo completo mergeado guardado en: qwen_QA_Sunat_merged/\")\n",
        "print(\"Este modelo puede ser cargado directamente con Transformers est谩ndar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwOfTlMuOam_",
        "outputId": "5c96a7ad-d01b-445c-955e-8a453e53b295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El RUC es el Registro nico de Contribuyentes, el cual identifica a los contribuyentes del IR, NRUS y el Nuevo R茅gimen nico Simplificado.\n"
          ]
        }
      ],
      "source": [
        "respuesta = preguntar_sunat(\"que es el RUC?\")\n",
        "print(respuesta)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
